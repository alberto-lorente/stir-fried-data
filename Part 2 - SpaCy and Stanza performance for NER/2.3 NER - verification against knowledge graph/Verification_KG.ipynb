{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(par_dir, '2.2 NER - analysis by entity type')\n",
    "\n",
    "stanza_dir = os.path.join(data_dir, \"df_entities_spacy_processed.csv\")\n",
    "spacy_dir = os.path.join(data_dir, \"df_entities_stanza_processed.csv\")\n",
    "\n",
    "entities_stanza = pd.read_csv(stanza_dir)\n",
    "entities_spacy = pd.read_csv(spacy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_people = list(set(list(np.unique(entities_spacy[\"subject\"])) + list(np.unique(entities_stanza[\"subject\"]))))\n",
    "list_dfs = [entities_spacy, entities_stanza]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regex(entity):\n",
    "    entity_extra = entity.split(\" \")\n",
    "    entity_new = \"_?\".join(entity_extra) # add optional _ since the entities usually appear that way in the graph\n",
    "    \n",
    "    return entity_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:49: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:49: SyntaxWarning: invalid escape sequence '\\('\n",
      "  ent_clean = str(ent).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:49: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  ent_clean = str(ent).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:49: SyntaxWarning: invalid escape sequence '\\('\n",
      "  ent_clean = str(ent).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:49: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  ent_clean = str(ent).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:50: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_clean = str(reg).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  regex_clean = str(reg).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:50: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_clean = str(reg).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9408\\1315109446.py:50: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  regex_clean = str(reg).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n"
     ]
    }
   ],
   "source": [
    "def verify_kg_person_package(person, df): \n",
    "    \n",
    "    # filtering by the subject of the bio\n",
    "    df_filter = df[df[\"subject\"] == person]\n",
    "    \n",
    "    # getting the category to know which Data Directory subfolder we have to access\n",
    "    category = df_filter[\"category\"].iloc[0]\n",
    "\n",
    "    # getting the entities found in the bios p\n",
    "    set_entities_df = set(df_filter[\"text\"].to_list())\n",
    "\n",
    "    \n",
    "    # total amount of unique entities\n",
    "    total_entities = len(set_entities_df)\n",
    "\n",
    "    # loading the json graphs to query them\n",
    "    par_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "    name_file_graph = person.replace(\" \", \"__\") + \".json\"\n",
    "    data_dir = os.path.join(par_dir, \"Data Directory\")\n",
    "    \n",
    "    if category == \"Chemistry\":\n",
    "        graph_dir = os.path.join(data_dir, \"chemistry_nobel_laureate\")\n",
    "    else:\n",
    "        graph_dir = os.path.join(data_dir, \"physics_nobel_laureate\")  \n",
    "    person_graph_path = os.path.join(graph_dir, name_file_graph)\n",
    "    with open(person_graph_path, \"r\") as f:\n",
    "        personal_graph = json.load(f)\n",
    "\n",
    "    list_subjects = [dict_graph[\"subject\"][\"value\"].strip(\"http://dbpedia.org/resource/\").rstrip() for dict_graph in personal_graph]\n",
    "    list_objects = [dict_graph[\"object\"][\"value\"].strip(\"http://dbpedia.org/resource/\").rstrip() for dict_graph in personal_graph]\n",
    "    \n",
    "    list_entities_graph = list(set(list_subjects + list_objects))\n",
    "    \n",
    "    regex_entities = [ent for ent in set_entities_df if ent.count(\"(\") == ent.count(\")\")]\n",
    "    \n",
    "    found_matches = 0\n",
    "    \n",
    "    # looking for the regexed entities in the graph\n",
    "    for ent in list_entities_graph:\n",
    "        match_search = []\n",
    "        for regex in regex_entities:\n",
    "            reg = make_regex(regex)\n",
    "            try:\n",
    "                # print(regex, ent)\n",
    "                match = re.search(reg, ent, flags=re.IGNORECASE)\n",
    "                match_search.append(match)\n",
    "            except:\n",
    "                try:\n",
    "                    ent_clean = str(ent).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
    "                    regex_clean = str(reg).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\").replace(\"\\\\\", \"\").replace(\"\\(\", \"\").replace(\"\\)\", \"\")\n",
    "                    match = re.search(regex_clean, ent_clean, flags=re.IGNORECASE)\n",
    "                    match_search.append(match)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "        match = [(search.group(0), search) for search in match_search if search != None]\n",
    "        if match:\n",
    "            found_matches = found_matches + 1\n",
    "\n",
    "    prop_matches = round((found_matches/total_entities), 2)\n",
    "    return prop_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_spacy = [verify_kg_person_package(person, entities_spacy) for person in list_people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can confidently say that around 9% of the entities detected by spaCy where present in the KG\n",
    "proportion_spacy = np.sum(results_spacy)/len(results_spacy)\n",
    "proportion_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stanza = [verify_kg_person_package(person, entities_stanza) for person in list_people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1052"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can confidently say that around 9% of the entities detected by Stanza where present in the KG\n",
    "proportion_stanza = np.sum(results_stanza)/len(results_stanza)\n",
    "proportion_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for person in list_people:\n",
    "    \n",
    "#     # filtering each df by the subject of the bio\n",
    "#     filter_df_spacy = entities_spacy[entities_spacy[\"subject\"] == person]\n",
    "#     filter_df_stanza = entities_stanza[entities_stanza[\"subject\"] == person]\n",
    "    \n",
    "#     # getting the category to know which Data Directory subfolder we have to access\n",
    "#     category = filter_df_spacy[\"category\"].iloc[0]\n",
    "    \n",
    "#     # print(category)\n",
    "#     # getting the entities found in the bios per package\n",
    "#     set_entities_spacy = set(filter_df_spacy[\"text\"].to_list())\n",
    "#     set_entities_stanza = set(filter_df_stanza[\"text\"].to_list())\n",
    "    \n",
    "#     # total amount of unique entities\n",
    "#     total_entities_spacy = len(set_entities_spacy)\n",
    "#     total_entities_stanza = len(set_entities_stanza)\n",
    "    \n",
    "#     found_entities_spacy = 0\n",
    "#     found_entities_stanza = 0   \n",
    "    \n",
    "#     # loading the json graphs to query them\n",
    "#     par_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "#     name_file_graph = person.replace(\" \", \"__\") + \".json\"\n",
    "#     data_dir = os.path.join(par_dir, \"Data Directory\")\n",
    "    \n",
    "#     if category == \"Chemistry\":\n",
    "#         graph_dir = os.path.join(data_dir, \"chemistry_nobel_laureate\")\n",
    "    \n",
    "#     else:\n",
    "#         graph_dir = os.path.join(data_dir, \"physics_nobel_laureate\")\n",
    "        \n",
    "#     person_graph_path = os.path.join(graph_dir, name_file_graph)\n",
    "#     # print(person_graph_path)\n",
    "    \n",
    "#     with open(person_graph_path, \"r\") as f:\n",
    "#         personal_graph = json.load(f)\n",
    "#     # if f:\n",
    "#     #     print(\"Graph load successfully!\")\n",
    "    \n",
    "#     list_subjects = [dict_graph[\"subject\"][\"value\"].strip(\"http://dbpedia.org/resource/\").rstrip() for dict_graph in personal_graph]\n",
    "#     list_objects = [dict_graph[\"object\"][\"value\"].strip(\"http://dbpedia.org/resource/\").rstrip() for dict_graph in personal_graph]\n",
    "    \n",
    "#     list_entities_graph = list(set(list_subjects + list_objects))\n",
    "    \n",
    "#     # print(list_entities_graph)\n",
    "    \n",
    "#     regex_entities_stanza = [ent for ent in set_entities_stanza if ent.count(\"(\") == ent.count(\")\")]\n",
    "#     regex_entities_spacy = [ent for ent in set_entities_spacy if ent.count(\"(\") == ent.count(\")\")]\n",
    "    \n",
    "#     matches_spacy = 0\n",
    "    \n",
    "#     for ent in list_entities_graph:\n",
    "#         # print(ent)\n",
    "#         # print(regex_entities_spacy)\n",
    "        \n",
    "#         match_search = []\n",
    "#         for regex in regex_entities_spacy:\n",
    "#             try:\n",
    "#                 # print(regex, ent)\n",
    "#                 match = re.search(regex, ent, flags=re.IGNORECASE)\n",
    "#                 match_search.append(match)\n",
    "#             except:\n",
    "#                 ent_clean = str(ent).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\")\n",
    "#                 regex_clean = str(regex).replace(\"[\", \"\").replace(\"]\", \"\").replace(\":\", \"\").replace(\"+\", \"\")\n",
    "#                 match = re.search(regex_clean, ent_clean, flags=re.IGNORECASE)\n",
    "#                 match_search.append(match)\n",
    "        \n",
    "#         match = [(search.group(0), search) for search in match_search if search != None]\n",
    "#         # print(ent)\n",
    "#         # print(match)\n",
    "#         if match:\n",
    "#             matches_spacy = matches_spacy + 1\n",
    "        \n",
    "#         # print(matches_spacy)\n",
    "#     prop_matches = round((matches_spacy/total_entities_spacy), 2)\n",
    "#     print(prop_matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
