{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set of NEs predicted by each package (i.e. Stanza and Spacy), how many of them can be found mentioned in the KG Graph you ollected for the person?7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return for each package (i.e. Stanza and Spacy), the ratio of\n",
    "predicted NEs that you can be confidently said to be in the KG\n",
    "graph for the person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 note that you should take into consid-\n",
    "eration the NE span in the text may not\n",
    "be an exact match with KG entity label.\n",
    "Here you can use regular expressions\n",
    "to match the NEs found in the text\n",
    "with the RDF entities present in the\n",
    "associated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(par_dir, '2.2 NER - analysis by entity type')\n",
    "\n",
    "stanza_dir = os.path.join(data_dir, \"df_entities_spacy_processed.csv\")\n",
    "spacy_dir = os.path.join(data_dir, \"df_entities_stanza_processed.csv\")\n",
    "\n",
    "entities_stanza = pd.read_csv(stanza_dir)\n",
    "entities_spacy = pd.read_csv(spacy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_people = list(set(list(np.unique(entities_spacy[\"subject\"])) + list(np.unique(entities_stanza[\"subject\"]))))\n",
    "list_entities = list(set(list(np.unique(entities_spacy[\"label\"])) + list(np.unique(entities_stanza[\"label\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in list_people:\n",
    "    \n",
    "    # filtering each df by the subject of the bio\n",
    "    filter_df_spacy = entities_spacy[entities_spacy[\"subject\"] == person]\n",
    "    filter_df_stanza = entities_stanza[entities_stanza[\"subject\"] == person]\n",
    "    \n",
    "    # getting the entities found in the bios per package\n",
    "    set_entities_spacy = set(filter_df_spacy[\"text\"].to_list())\n",
    "    set_entities_stanza = set(filter_df_stanza[\"text\"].to_list())\n",
    "    \n",
    "    # total amount of unique entities\n",
    "    total_entities_spacy = len(set_entities_spacy)\n",
    "    total_entities_stanza = len(set_entities_stanza)\n",
    "    \n",
    "    found_entities_spacy = 0\n",
    "    found_entities_stanza = 0\n",
    "    \n",
    "    ########### need to load each json per person individually here and \n",
    "    # query the predicates with sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
