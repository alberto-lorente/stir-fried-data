{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:55:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 24.6MB/s]                    \n",
      "2024-06-13 11:55:49 INFO: Downloaded file to /Users/tunji/stanza_resources/resources.json\n",
      "2024-06-13 11:55:49 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-06-13 11:55:50 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-06-13 11:55:50 INFO: Using device: cpu\n",
      "2024-06-13 11:55:50 INFO: Loading: tokenize\n",
      "2024-06-13 11:55:50 INFO: Loading: mwt\n",
      "2024-06-13 11:55:50 INFO: Loading: ner\n",
      "2024-06-13 11:55:50 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import stanza\n",
    "\n",
    "data_dir = os.getcwd()\n",
    "data_dir = os.path.join(data_dir, \"..\", \"..\", \"Data Directory\")\n",
    "\n",
    "# get the list of tags from Stanza to double-check that they are the same as the SpaCy ones\n",
    "nlp_stanza = stanza.Pipeline('en',processors= 'tokenize,ner')\n",
    "tags_stanza = list(nlp_stanza.processors['ner'].get_known_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# get the list of tags from SpaCy to double-check that they are the same as the Stanza ones\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tags_spacy = list(nlp.get_pipe('ner').labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# they are indeed the same!\n",
    "tags_spacy == tags_stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_spacy = pd.read_csv(os.path.join(data_dir, \"df_entities_spacy_processed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_stanza = pd.read_csv(os.path.join(data_dir, \"df_entities_stanza_processed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a list of people we will use to iterate over the dataframes\n",
    "list_people = np.unique(df_entities_spacy[\"subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'n_word', 'start_pos', 'end_pos', 'label', 'biography',\n",
       "       'subject', 'source_index', 'category', 'range_span'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entities_spacy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NORP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 60\u001b[0m\n\u001b[1;32m     51\u001b[0m total_agreement \u001b[38;5;241m=\u001b[39m total_agreement \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(pos_token_spacy)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(pos_token_stanza)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# print(dict_ners[pos_token_spacy])\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(dict_ners[pos_token_stanza])\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print(dict_ners)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m dict_ners[pos_token_spacy][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdict_ners\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_token_spacy\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     61\u001b[0m dict_ners[pos_token_stanza][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dict_ners[pos_token_stanza][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# checking the labels for the entities are the same\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NORP'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for person in list_people:\n",
    "    \n",
    "    # filter the dataframes by the person to get granular results per biography\n",
    "    filter_df_spacy = df_entities_spacy[df_entities_spacy[\"subject\"] == person][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n",
    "    filter_df_stanza = df_entities_stanza[df_entities_stanza[\"subject\"] == person][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n",
    "    \n",
    "    # initiating indexes for the spacy and stanza filter dfs\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    partial_agreement = 0\n",
    "    total_agreement = 0\n",
    "    label_agreement = 0\n",
    "\n",
    "    dict_ners = {}\n",
    "    for ner in list_ners:\n",
    "        dict_ners[ner] = {\"Total\": 0, \"Agreed\": 0}\n",
    "    \n",
    "    # print(dict_ners)\n",
    "    # print(dict_ners.keys())\n",
    "\n",
    "    # iterate over the rows of the filter dfs as long as there are rows left to check in BOTH of them\n",
    "    while i+1 < len(filter_df_spacy) and j+1 < len(filter_df_stanza): # adjusting for mismatch between indexing and length\n",
    "\n",
    "        # print(i, j)\n",
    "        token_text_spacy = filter_df_spacy.iloc[i][\"text\"]\n",
    "        token_text_stanza = filter_df_stanza.iloc[j][\"text\"]\n",
    "        \n",
    "        # print(token_text_spacy, token_text_stanza)\n",
    "        # print(len(token_text_spacy), len(token_text_stanza))\n",
    "        \n",
    "        # using list objects in pandas is a bit complicated since they are not saved in list type\n",
    "        # these two lines are manually reconstructing the list object\n",
    "        token_span_spacy = filter_df_spacy.iloc[i][\"range_span\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \"\").replace(\"  \", \" \").strip().split(\" \")\n",
    "        token_span_stanza = filter_df_stanza.iloc[j][\"range_span\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \"\").replace(\"  \", \" \").strip().split(\" \")\n",
    "\n",
    "        # print(token_span_spacy, token_span_stanza)\n",
    "        # print(len(token_span_spacy), len(token_span_stanza))\n",
    "\n",
    "        pos_token_spacy = filter_df_spacy.iloc[i][\"label\"]\n",
    "        pos_token_stanza = filter_df_stanza.iloc[j][\"label\"]\n",
    "\n",
    "        # checking if the entities have overlapping indexes, that is, if they are partial matches the list will be of length > 1\n",
    "        list_agreement_span = [index for index in token_span_spacy if index in token_span_spacy and index in token_span_stanza]\n",
    "        \n",
    "        # if the spans are the same, the entities will be the same and we will have a total agreement\n",
    "        if token_span_spacy == token_span_stanza:\n",
    "            # print(token_text_spacy, token_text_stanza)\n",
    "            total_agreement = total_agreement + 1\n",
    "            \n",
    "            # print(pos_token_spacy)\n",
    "            # print(pos_token_stanza)\n",
    "            # print(dict_ners[pos_token_spacy])\n",
    "            # print(dict_ners[pos_token_stanza])\n",
    "            # print(dict_ners)\n",
    "            \n",
    "            \n",
    "            dict_ners[pos_token_spacy][\"Total\"] = dict_ners[pos_token_spacy][\"Total\"] + 1\n",
    "            dict_ners[pos_token_stanza][\"Total\"] = dict_ners[pos_token_stanza][\"Total\"] + 1\n",
    "            \n",
    "            # checking the labels for the entities are the same\n",
    "            if pos_token_spacy == pos_token_stanza:\n",
    "                label_agreement = label_agreement + 1\n",
    "                \n",
    "                dict_ners[pos_token_spacy][\"Agreed\"] = dict_ners[pos_token_spacy][\"Agreed\"] + 1\n",
    "                dict_ners[pos_token_stanza][\"Agreed\"] = dict_ners[pos_token_stanza][\"Agreed\"] + 1\n",
    "            \n",
    "            # we advance the indexes of both filter dfs\n",
    "            i = i + 1\n",
    "            j = j + 1\n",
    "            \n",
    "        # if there are common indexes in the span\n",
    "        elif len(list_agreement_span) > 1:\n",
    "            # print(token_text_spacy, token_text_stanza)\n",
    "            partial_agreement = partial_agreement + 1 \n",
    "            \n",
    "            dict_ners[pos_token_spacy][\"Total\"] = dict_ners[pos_token_spacy][\"Total\"] + 1\n",
    "            dict_ners[pos_token_stanza][\"Total\"] = dict_ners[pos_token_stanza][\"Total\"] + 1\n",
    "            \n",
    "            # checking the labels for the entities are the same\n",
    "            if pos_token_spacy == pos_token_stanza:\n",
    "                label_agreement = label_agreement + 1\n",
    "                \n",
    "                dict_ners[pos_token_spacy][\"Agreed\"] = dict_ners[pos_token_spacy][\"Agreed\"] + 1\n",
    "                dict_ners[pos_token_stanza][\"Agreed\"] = dict_ners[pos_token_stanza][\"Agreed\"] + 1\n",
    "            \n",
    "            # we advance the indexes of both filter dfs    \n",
    "            i = i + 1\n",
    "            j = j + 1\n",
    "        \n",
    "        # if there is no agreement we will advance the indexes differently; \n",
    "        else:\n",
    "            # if the ending spam of the entity of spacy is smaller than the one in stanza, \n",
    "            # that means that stanza is \"further\" along the text so even if these indexes did not have agreement in entities,\n",
    "            # the following index in the spacy df could be a match with the current index of the stanza df hence we have to check them too\n",
    "            if token_span_spacy[-1] < token_span_stanza[-1]:\n",
    "                i = i + 1\n",
    "                j = j\n",
    "            # for the reverse case\n",
    "            elif token_span_spacy[-1] > token_span_stanza[-1]:\n",
    "                i = i\n",
    "                j = j + 1\n",
    "                \n",
    "        # print(dict_ners)\n",
    "        # print(dict_ners.keys())\n",
    "        # print(\"DONE WITH TOKENS\", token_text_spacy, token_text_stanza)\n",
    "        \n",
    "    total = len(filter_df_spacy) + len(filter_df_stanza) - partial_agreement - total_agreement\n",
    "    per_agreement = round(((total_agreement+partial_agreement)/total), 2)\n",
    "    \n",
    "    # saving the results in a dictionary which will later be turned into a df for analysis\n",
    "    results_person = {\"subject\":person,\n",
    "                      \"total_entities\": total,\n",
    "                      \"percent_partial_or_total_agreement_span\":per_agreement,  # over the total amount of entities\n",
    "                      \"total_agreement_per\": round((total_agreement/total), 2),  # over the total amount of entities\n",
    "                      \"partial_agreement_per\": round((partial_agreement/total), 2),  # over the total amount of entities\n",
    "                      \"ner_breakdown\": dict_ners}\n",
    "    \n",
    "    # getting aggregate results for the entity types\n",
    "    for key in results_person[\"ner_breakdown\"].keys():\n",
    "        results_person[\"total_ner_aggregate\"] = round((sum([results_person[\"ner_breakdown\"][key][\"Agreed\"] for key in results_person[\"ner_breakdown\"].keys()])  / sum([results_person[\"ner_breakdown\"][key][\"Total\"] for key in results_person[\"ner_breakdown\"].keys()])), 2)\n",
    "        if results_person[\"ner_breakdown\"][key][\"Total\"] != 0: # avoiding zero division\n",
    "            agreement_label_perc = round((results_person[\"ner_breakdown\"][key][\"Agreed\"] / results_person[\"ner_breakdown\"][key][\"Total\"]), 2)\n",
    "            results_person[\"ner_breakdown\"][key][\"agreement_per\"] = agreement_label_perc\n",
    "            \n",
    "        else:\n",
    "            results_person[\"ner_breakdown\"][key][\"agreement_per\"] = \"Not applicable\"\n",
    "            \n",
    "    results.append(results_person)\n",
    "    \n",
    "    # print(person)\n",
    "    # print(\" Same annotations:\", total_agreement)\n",
    "    # print(\" Partial annotations:\", partial_agreement)\n",
    "    # print(\" Total:\", total)\n",
    "    # print(\" Perc. total or partial annotations:\", per_agreement)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Aage Bohr',\n",
       " 'total_entities': 328,\n",
       " 'percent_partial_or_total_agreement_span': 0.76,\n",
       " 'total_agreement_per': 0.68,\n",
       " 'partial_agreement_per': 0.08,\n",
       " 'ner_breakdown': {'CARDINAL': {'Total': 26,\n",
       "   'Agreed': 22,\n",
       "   'agreement_per': 0.85},\n",
       "  'DATE': {'Total': 125, 'Agreed': 122, 'agreement_per': 0.98},\n",
       "  'EVENT': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'FAC': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'GPE': {'Total': 66, 'Agreed': 42, 'agreement_per': 0.64},\n",
       "  'LANGUAGE': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'LAW': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'LOC': {'Total': 1, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'MONEY': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'NORP': {'Total': 26, 'Agreed': 26, 'agreement_per': 1.0},\n",
       "  'ORDINAL': {'Total': 10, 'Agreed': 10, 'agreement_per': 1.0},\n",
       "  'ORG': {'Total': 84, 'Agreed': 62, 'agreement_per': 0.74},\n",
       "  'PERCENT': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'PERSON': {'Total': 125, 'Agreed': 100, 'agreement_per': 0.8},\n",
       "  'PRODUCT': {'Total': 2, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'QUANTITY': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'TIME': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'WORK_OF_ART': {'Total': 29, 'Agreed': 16, 'agreement_per': 0.55}},\n",
       " 'total_ner_aggregate': 0.81}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the revelant info we will analyse\n",
    "keys_aggregate = [\"percent_partial_or_total_agreement_span\", \"total_agreement_per\", \"partial_agreement_per\", \"total_ner_aggregate\"]\n",
    "results_aggregate_clean = []\n",
    "for result in results:\n",
    "    dict_result = {k:v for k,v in result.items() if k in keys_aggregate}\n",
    "    for k, val in result[\"ner_breakdown\"].items():\n",
    "        dict_result[k] = result[\"ner_breakdown\"][k][\"agreement_per\"]\n",
    "    results_aggregate_clean.append(dict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_aggregate_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_aggregate_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the df to analyse it in the next notebook\n",
    "df_results.to_csv(os.path.join(data_dir, \"results_ner_comparison.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
