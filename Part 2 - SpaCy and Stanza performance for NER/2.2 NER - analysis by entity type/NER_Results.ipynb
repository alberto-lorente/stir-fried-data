{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stanza\n",
    "# # stanza.download('en')\n",
    "\n",
    "# nlp_stanza = stanza.Pipeline('en',processors= 'tokenize,ner')\n",
    "# nlp_stanza.processors['ner'].get_known_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ners = list(nlp.get_pipe('ner').labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_spacy = pd.read_csv(r\"df_entities_spacy_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_stanza = pd.read_csv(r\"df_entities_stanza_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_people = np.unique(df_entities_spacy[\"subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'n_word', 'start_pos', 'end_pos', 'label', 'biography',\n",
       "       'subject', 'source_index', 'category', 'range_span'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entities_spacy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, just need to complete it a lil bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df_spacy = df_entities_spacy[df_entities_spacy[\"subject\"] == list_people[0]][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df_stanza = df_entities_stanza[df_entities_stanza[\"subject\"] == list_people[0]][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_df_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_df_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for person in list_people:\n",
    "    \n",
    "    filter_df_spacy = df_entities_spacy[df_entities_spacy[\"subject\"] == person][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n",
    "    filter_df_stanza = df_entities_stanza[df_entities_stanza[\"subject\"] == person][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    partial_agreement = 0\n",
    "    total_agreement = 0\n",
    "    label_agreement = 0\n",
    "    \n",
    "    # doing this with list comprehendion made it so that all the nested dictionaries would update when one updated\n",
    "    dict_ners = {}\n",
    "    for ner in list_ners:\n",
    "        dict_ners[ner] = {\"Total\": 0, \"Agreed\": 0}\n",
    "    \n",
    "    # print(dict_ners)\n",
    "    # print(dict_ners.keys())\n",
    "\n",
    "    while i+1 < len(filter_df_spacy) and j+1 < len(filter_df_stanza): # adjusting for mismatch between indexing and length\n",
    "\n",
    "        # print(i, j)\n",
    "        token_text_spacy = filter_df_spacy.iloc[i][\"text\"]\n",
    "        token_text_stanza = filter_df_stanza.iloc[j][\"text\"]\n",
    "        \n",
    "        # print(token_text_spacy, token_text_stanza)\n",
    "        # print(len(token_text_spacy), len(token_text_stanza))\n",
    "        \n",
    "        token_span_spacy = filter_df_spacy.iloc[i][\"range_span\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \"\").replace(\"  \", \" \").strip().split(\" \")\n",
    "        token_span_stanza = filter_df_stanza.iloc[j][\"range_span\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \"\").replace(\"  \", \" \").strip().split(\" \")\n",
    "\n",
    "        # print(token_span_spacy, token_span_stanza)\n",
    "        # print(len(token_span_spacy), len(token_span_stanza))\n",
    "\n",
    "        pos_token_spacy = filter_df_spacy.iloc[i][\"label\"]\n",
    "        pos_token_stanza = filter_df_stanza.iloc[j][\"label\"]\n",
    "\n",
    "        list_agreement_span = [index for index in token_span_spacy if index in token_span_spacy and index in token_span_stanza]\n",
    "        \n",
    "        if token_span_spacy == token_span_stanza:\n",
    "            # print(token_text_spacy, token_text_stanza)\n",
    "            total_agreement = total_agreement + 1\n",
    "            \n",
    "            # print(pos_token_spacy)\n",
    "            # print(pos_token_stanza)\n",
    "            # print(dict_ners[pos_token_spacy])\n",
    "            # print(dict_ners[pos_token_stanza])\n",
    "            # print(dict_ners)\n",
    "            \n",
    "            dict_ners[pos_token_spacy][\"Total\"] = dict_ners[pos_token_spacy][\"Total\"] + 1\n",
    "            dict_ners[pos_token_stanza][\"Total\"] = dict_ners[pos_token_stanza][\"Total\"] + 1\n",
    "            \n",
    "            if pos_token_spacy == pos_token_stanza:\n",
    "                label_agreement = label_agreement + 1\n",
    "                \n",
    "                dict_ners[pos_token_spacy][\"Agreed\"] = dict_ners[pos_token_spacy][\"Agreed\"] + 1\n",
    "                dict_ners[pos_token_stanza][\"Agreed\"] = dict_ners[pos_token_stanza][\"Agreed\"] + 1\n",
    "                \n",
    "            i = i + 1\n",
    "            j = j + 1\n",
    "            \n",
    "            \n",
    "        elif len(list_agreement_span) > 1:\n",
    "            # print(token_text_spacy, token_text_stanza)\n",
    "            partial_agreement = partial_agreement + 1 \n",
    "            \n",
    "            dict_ners[pos_token_spacy][\"Total\"] = dict_ners[pos_token_spacy][\"Total\"] + 1\n",
    "            dict_ners[pos_token_stanza][\"Total\"] = dict_ners[pos_token_stanza][\"Total\"] + 1\n",
    "            \n",
    "            if pos_token_spacy == pos_token_stanza:\n",
    "                label_agreement = label_agreement + 1\n",
    "                \n",
    "                dict_ners[pos_token_spacy][\"Agreed\"] = dict_ners[pos_token_spacy][\"Agreed\"] + 1\n",
    "                dict_ners[pos_token_stanza][\"Agreed\"] = dict_ners[pos_token_stanza][\"Agreed\"] + 1\n",
    "                \n",
    "            i = i + 1\n",
    "            j = j + 1\n",
    "            \n",
    "        else:\n",
    "            if token_span_spacy[-1] < token_span_stanza[-1]:\n",
    "                i = i + 1\n",
    "                j = j\n",
    "            elif token_span_spacy[-1] > token_span_stanza[-1]:\n",
    "                i = i\n",
    "                j = j + 1\n",
    "                \n",
    "        # print(dict_ners)\n",
    "        # print(dict_ners.keys())\n",
    "        # print(\"DONE WITH TOKENS\", token_text_spacy, token_text_stanza)\n",
    "        \n",
    "    total = len(filter_df_spacy) + len(filter_df_stanza) - partial_agreement - total_agreement\n",
    "    per_agreement = round(((total_agreement+partial_agreement)/total), 2)\n",
    "    \n",
    "    results_person = {\"subject\":person,\n",
    "                      \"total_entities\": total,\n",
    "                      \"percent_partial_or_total_agreement_span\":per_agreement,  # over the total amount of entities\n",
    "                      \"total_agreement_per\": round((total_agreement/total), 2),  # over the total amount of entities\n",
    "                      \"partial_agreement_per\": round((partial_agreement/total), 2),  # over the total amount of entities\n",
    "                      \"ner_breakdown\": dict_ners}\n",
    "    \n",
    "    for key in results_person[\"ner_breakdown\"].keys():\n",
    "        results_person[\"total_ner_aggregate\"] = round((sum([results_person[\"ner_breakdown\"][key][\"Agreed\"] for key in results_person[\"ner_breakdown\"].keys()])  / sum([results_person[\"ner_breakdown\"][key][\"Total\"] for key in results_person[\"ner_breakdown\"].keys()])), 2)\n",
    "        if results_person[\"ner_breakdown\"][key][\"Total\"] != 0: # avoiding zero division\n",
    "            agreement_label_perc = round((results_person[\"ner_breakdown\"][key][\"Agreed\"] / results_person[\"ner_breakdown\"][key][\"Total\"]), 2)\n",
    "            results_person[\"ner_breakdown\"][key][\"agreement_per\"] = agreement_label_perc\n",
    "            \n",
    "        else:\n",
    "            results_person[\"ner_breakdown\"][key][\"agreement_per\"] = \"Not applicable\"\n",
    "            \n",
    "    results.append(results_person)\n",
    "    \n",
    "    # print(person)\n",
    "    # print(\" Same annotations:\", total_agreement)\n",
    "    # print(\" Partial annotations:\", partial_agreement)\n",
    "    # print(\" Total:\", total)\n",
    "    # print(\" Perc. total or partial annotations:\", per_agreement)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Aage Bohr',\n",
       " 'total_entities': 328,\n",
       " 'percent_partial_or_total_agreement_span': 0.76,\n",
       " 'total_agreement_per': 0.68,\n",
       " 'partial_agreement_per': 0.08,\n",
       " 'ner_breakdown': {'CARDINAL': {'Total': 26,\n",
       "   'Agreed': 22,\n",
       "   'agreement_per': 0.85},\n",
       "  'DATE': {'Total': 125, 'Agreed': 122, 'agreement_per': 0.98},\n",
       "  'EVENT': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'FAC': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'GPE': {'Total': 66, 'Agreed': 42, 'agreement_per': 0.64},\n",
       "  'LANGUAGE': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'LAW': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'LOC': {'Total': 1, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'MONEY': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'NORP': {'Total': 26, 'Agreed': 26, 'agreement_per': 1.0},\n",
       "  'ORDINAL': {'Total': 10, 'Agreed': 10, 'agreement_per': 1.0},\n",
       "  'ORG': {'Total': 84, 'Agreed': 62, 'agreement_per': 0.74},\n",
       "  'PERCENT': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'PERSON': {'Total': 125, 'Agreed': 100, 'agreement_per': 0.8},\n",
       "  'PRODUCT': {'Total': 2, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'QUANTITY': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'TIME': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'WORK_OF_ART': {'Total': 29, 'Agreed': 16, 'agreement_per': 0.55}},\n",
       " 'total_ner_aggregate': 0.81}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_aggregate = [\"percent_partial_or_total_agreement_span\", \"total_agreement_per\", \"partial_agreement_per\", \"total_ner_aggregate\"]\n",
    "results_aggregate_clean = []\n",
    "for result in results:\n",
    "    dict_result = {k:v for k,v in result.items() if k in keys_aggregate}\n",
    "    for k, val in result[\"ner_breakdown\"].items():\n",
    "        dict_result[k] = result[\"ner_breakdown\"][k][\"agreement_per\"]\n",
    "    results_aggregate_clean.append(dict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_aggregate_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_aggregate_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results_ner_comparison.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
