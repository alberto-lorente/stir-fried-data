{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-13 21:47:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 6.12MB/s]                    \n",
      "2024-06-13 21:47:06 INFO: Downloaded file to C:\\Users\\alber\\stanza_resources\\resources.json\n",
      "2024-06-13 21:47:06 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-06-13 21:47:06 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-06-13 21:47:06 INFO: Using device: cuda\n",
      "2024-06-13 21:47:06 INFO: Loading: tokenize\n",
      "2024-06-13 21:47:07 INFO: Loading: mwt\n",
      "2024-06-13 21:47:07 INFO: Loading: ner\n",
      "2024-06-13 21:47:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import stanza\n",
    "\n",
    "# get the list of tags from Stanza to double-check that they are the same as the SpaCy ones\n",
    "nlp_stanza = stanza.Pipeline('en',processors= 'tokenize,ner')\n",
    "tags_stanza = list(nlp_stanza.processors['ner'].get_known_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# get the list of tags from SpaCy to double-check that they are the same as the Stanza ones\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tags_spacy = list(nlp.get_pipe('ner').labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# they are indeed the same!\n",
    "tags_spacy == tags_stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_spacy = pd.read_csv(os.path.join(os.getcwd(), \"df_entities_spacy_processed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_stanza = pd.read_csv(os.path.join(os.getcwd(), \"df_entities_stanza_processed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a list of people we will use to iterate over the dataframes\n",
    "list_people = np.unique(df_entities_spacy[\"subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'n_word', 'start_pos', 'end_pos', 'label', 'biography',\n",
       "       'subject', 'source_index', 'category', 'range_span'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entities_spacy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for person in list_people:\n",
    "    \n",
    "    # filter the dataframes by the person to get granular results per biography\n",
    "    filter_df_spacy = df_entities_spacy[df_entities_spacy[\"subject\"] == person][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n",
    "    filter_df_stanza = df_entities_stanza[df_entities_stanza[\"subject\"] == person][[\"subject\", \"text\", \"label\", \"start_pos\", \"range_span\"]]\n",
    "    \n",
    "    # initiating indexes for the spacy and stanza filter dfs\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    partial_agreement = 0\n",
    "    total_agreement = 0\n",
    "    label_agreement = 0\n",
    "\n",
    "    dict_ners = {}\n",
    "    for ner in tags_spacy:\n",
    "        dict_ners[ner] = {\"Total\": 0, \"Agreed\": 0}\n",
    "    \n",
    "    # print(dict_ners)\n",
    "    # print(dict_ners.keys())\n",
    "\n",
    "    # iterate over the rows of the filter dfs as long as there are rows left to check in BOTH of them\n",
    "    while i+1 < len(filter_df_spacy) and j+1 < len(filter_df_stanza): # adjusting for mismatch between indexing and length\n",
    "\n",
    "        # print(i, j)\n",
    "        token_text_spacy = filter_df_spacy.iloc[i][\"text\"]\n",
    "        token_text_stanza = filter_df_stanza.iloc[j][\"text\"]\n",
    "        \n",
    "        # print(token_text_spacy, token_text_stanza)\n",
    "        # print(len(token_text_spacy), len(token_text_stanza))\n",
    "        \n",
    "        # using list objects in pandas is a bit complicated since they are not saved in list type\n",
    "        # these two lines are manually reconstructing the list object\n",
    "        token_span_spacy = filter_df_spacy.iloc[i][\"range_span\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \"\").replace(\"  \", \" \").strip().split(\" \")\n",
    "        token_span_stanza = filter_df_stanza.iloc[j][\"range_span\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \"\").replace(\"  \", \" \").strip().split(\" \")\n",
    "\n",
    "        # print(token_span_spacy, token_span_stanza)\n",
    "        # print(len(token_span_spacy), len(token_span_stanza))\n",
    "\n",
    "        pos_token_spacy = filter_df_spacy.iloc[i][\"label\"]\n",
    "        pos_token_stanza = filter_df_stanza.iloc[j][\"label\"]\n",
    "\n",
    "        # checking if the entities have overlapping indexes, that is, if they are partial matches the list will be of length > 1\n",
    "        list_agreement_span = [index for index in token_span_spacy if index in token_span_spacy and index in token_span_stanza]\n",
    "        \n",
    "        # if the spans are the same, the entities will be the same and we will have a total agreement\n",
    "        if token_span_spacy == token_span_stanza:\n",
    "            # print(token_text_spacy, token_text_stanza)\n",
    "            total_agreement = total_agreement + 1\n",
    "            \n",
    "            # print(pos_token_spacy)\n",
    "            # print(pos_token_stanza)\n",
    "            # print(dict_ners[pos_token_spacy])\n",
    "            # print(dict_ners[pos_token_stanza])\n",
    "            # print(dict_ners)\n",
    "            \n",
    "            \n",
    "            dict_ners[pos_token_spacy][\"Total\"] = dict_ners[pos_token_spacy][\"Total\"] + 1\n",
    "            dict_ners[pos_token_stanza][\"Total\"] = dict_ners[pos_token_stanza][\"Total\"] + 1\n",
    "            \n",
    "            # checking the labels for the entities are the same\n",
    "            if pos_token_spacy == pos_token_stanza:\n",
    "                label_agreement = label_agreement + 1\n",
    "                \n",
    "                dict_ners[pos_token_spacy][\"Agreed\"] = dict_ners[pos_token_spacy][\"Agreed\"] + 1\n",
    "                dict_ners[pos_token_stanza][\"Agreed\"] = dict_ners[pos_token_stanza][\"Agreed\"] + 1\n",
    "            \n",
    "            # we advance the indexes of both filter dfs\n",
    "            i = i + 1\n",
    "            j = j + 1\n",
    "            \n",
    "        # if there are common indexes in the span\n",
    "        elif len(list_agreement_span) > 1:\n",
    "            # print(token_text_spacy, token_text_stanza)\n",
    "            partial_agreement = partial_agreement + 1 \n",
    "            \n",
    "            dict_ners[pos_token_spacy][\"Total\"] = dict_ners[pos_token_spacy][\"Total\"] + 1\n",
    "            dict_ners[pos_token_stanza][\"Total\"] = dict_ners[pos_token_stanza][\"Total\"] + 1\n",
    "            \n",
    "            # checking the labels for the entities are the same\n",
    "            if pos_token_spacy == pos_token_stanza:\n",
    "                label_agreement = label_agreement + 1\n",
    "                \n",
    "                dict_ners[pos_token_spacy][\"Agreed\"] = dict_ners[pos_token_spacy][\"Agreed\"] + 1\n",
    "                dict_ners[pos_token_stanza][\"Agreed\"] = dict_ners[pos_token_stanza][\"Agreed\"] + 1\n",
    "            \n",
    "            # we advance the indexes of both filter dfs    \n",
    "            i = i + 1\n",
    "            j = j + 1\n",
    "        \n",
    "        # if there is no agreement we will advance the indexes differently; \n",
    "        else:\n",
    "            # if the ending spam of the entity of spacy is smaller than the one in stanza, \n",
    "            # that means that stanza is \"further\" along the text so even if these indexes did not have agreement in entities,\n",
    "            # the following index in the spacy df could be a match with the current index of the stanza df hence we have to check them too\n",
    "            if token_span_spacy[-1] < token_span_stanza[-1]:\n",
    "                i = i + 1\n",
    "                j = j\n",
    "            # for the reverse case\n",
    "            elif token_span_spacy[-1] > token_span_stanza[-1]:\n",
    "                i = i\n",
    "                j = j + 1\n",
    "                \n",
    "        # print(dict_ners)\n",
    "        # print(dict_ners.keys())\n",
    "        # print(\"DONE WITH TOKENS\", token_text_spacy, token_text_stanza)\n",
    "        \n",
    "    total = len(filter_df_spacy) + len(filter_df_stanza) - partial_agreement - total_agreement\n",
    "    per_agreement = round(((total_agreement+partial_agreement)/total), 2)\n",
    "    \n",
    "    # saving the results in a dictionary which will later be turned into a df for analysis\n",
    "    results_person = {\"subject\":person,\n",
    "                      \"total_entities\": total,\n",
    "                      \"percent_partial_or_total_agreement_span\":per_agreement,  # over the total amount of entities\n",
    "                      \"total_agreement_per\": round((total_agreement/total), 2),  # over the total amount of entities\n",
    "                      \"partial_agreement_per\": round((partial_agreement/total), 2),  # over the total amount of entities\n",
    "                      \"ner_breakdown\": dict_ners}\n",
    "    \n",
    "    # getting aggregate results for the entity types\n",
    "    for key in results_person[\"ner_breakdown\"].keys():\n",
    "        results_person[\"total_ner_aggregate\"] = round((sum([results_person[\"ner_breakdown\"][key][\"Agreed\"] for key in results_person[\"ner_breakdown\"].keys()])  / sum([results_person[\"ner_breakdown\"][key][\"Total\"] for key in results_person[\"ner_breakdown\"].keys()])), 2)\n",
    "        if results_person[\"ner_breakdown\"][key][\"Total\"] != 0: # avoiding zero division\n",
    "            agreement_label_perc = round((results_person[\"ner_breakdown\"][key][\"Agreed\"] / results_person[\"ner_breakdown\"][key][\"Total\"]), 2)\n",
    "            results_person[\"ner_breakdown\"][key][\"agreement_per\"] = agreement_label_perc\n",
    "            \n",
    "        else:\n",
    "            results_person[\"ner_breakdown\"][key][\"agreement_per\"] = \"Not applicable\"\n",
    "            \n",
    "    results.append(results_person)\n",
    "    \n",
    "    # print(person)\n",
    "    # print(\" Same annotations:\", total_agreement)\n",
    "    # print(\" Partial annotations:\", partial_agreement)\n",
    "    # print(\" Total:\", total)\n",
    "    # print(\" Perc. total or partial annotations:\", per_agreement)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Aage Bohr',\n",
       " 'total_entities': 407,\n",
       " 'percent_partial_or_total_agreement_span': 0.42,\n",
       " 'total_agreement_per': 0.02,\n",
       " 'partial_agreement_per': 0.4,\n",
       " 'ner_breakdown': {'CARDINAL': {'Total': 17,\n",
       "   'Agreed': 8,\n",
       "   'agreement_per': 0.47},\n",
       "  'DATE': {'Total': 66, 'Agreed': 22, 'agreement_per': 0.33},\n",
       "  'EVENT': {'Total': 1, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'FAC': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'GPE': {'Total': 41, 'Agreed': 12, 'agreement_per': 0.29},\n",
       "  'LANGUAGE': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'LAW': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'LOC': {'Total': 1, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'MONEY': {'Total': 1, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'NORP': {'Total': 9, 'Agreed': 6, 'agreement_per': 0.67},\n",
       "  'ORDINAL': {'Total': 4, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'ORG': {'Total': 80, 'Agreed': 46, 'agreement_per': 0.57},\n",
       "  'PERCENT': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'PERSON': {'Total': 90, 'Agreed': 52, 'agreement_per': 0.58},\n",
       "  'PRODUCT': {'Total': 1, 'Agreed': 0, 'agreement_per': 0.0},\n",
       "  'QUANTITY': {'Total': 0, 'Agreed': 0, 'agreement_per': 'Not applicable'},\n",
       "  'TIME': {'Total': 2, 'Agreed': 2, 'agreement_per': 1.0},\n",
       "  'WORK_OF_ART': {'Total': 27, 'Agreed': 10, 'agreement_per': 0.37}},\n",
       " 'total_ner_aggregate': 0.47}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the revelant info we will analyse\n",
    "keys_aggregate = [\"percent_partial_or_total_agreement_span\", \"total_agreement_per\", \"partial_agreement_per\", \"total_ner_aggregate\"]\n",
    "results_aggregate_clean = []\n",
    "for result in results:\n",
    "    dict_result = {k:v for k,v in result.items() if k in keys_aggregate}\n",
    "    for k, val in result[\"ner_breakdown\"].items():\n",
    "        dict_result[k] = result[\"ner_breakdown\"][k][\"agreement_per\"]\n",
    "    results_aggregate_clean.append(dict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_aggregate_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_aggregate_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the df to analyse it in the next notebook\n",
    "df_results.to_csv(\"results_ner_comparison.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
